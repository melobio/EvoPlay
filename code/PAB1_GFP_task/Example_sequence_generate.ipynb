{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e01ec58",
   "metadata": {},
   "source": [
    "## EvoPlay序列优化生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f0679",
   "metadata": {},
   "source": [
    "### 导入相关package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c550f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T07:53:14.168708Z",
     "start_time": "2023-06-09T07:53:11.487490Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Htang/miniconda3/envs/lm_gvp/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "from sequence_env_m_p import Seq_env, Mutate\n",
    "from mcts_alphaZero_mutate_expand_m_p_gfp import MCTSMutater\n",
    "# from p_v_net_torch import PolicyValueNet  # Pytorch\n",
    "from p_v_net_3 import PolicyValueNet\n",
    "from env_model import CNN2\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Union\n",
    "import sys\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de167c17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T07:53:17.634519Z",
     "start_time": "2023-06-09T07:53:17.538712Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def string_to_one_hot(sequence: str, alphabet: str) -> np.ndarray:\n",
    "\n",
    "    out = np.zeros((len(sequence), len(alphabet)))\n",
    "    for i in range(len(sequence)):\n",
    "        out[i, alphabet.index(sequence[i])] = 1\n",
    "    return out\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq, target = self.sequences[index], self.labels[index]\n",
    "        return seq, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "def one_hot_to_string(\n",
    "    one_hot: Union[List[List[int]], np.ndarray], alphabet: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return the sequence string representing a one-hot vector according to an alphabet.\n",
    "\n",
    "    Args:\n",
    "        one_hot: One-hot of shape `(len(sequence), len(alphabet)` representing\n",
    "            a sequence.\n",
    "        alphabet: Alphabet string (assigns each character an index).\n",
    "\n",
    "    Returns:\n",
    "        Sequence string representation of `one_hot`.\n",
    "\n",
    "    \"\"\"\n",
    "    residue_idxs = np.argmax(one_hot, axis=1)\n",
    "    return \"\".join([alphabet[idx] for idx in residue_idxs])\n",
    "\n",
    "def raw_to_features(data_dir,part=1):\n",
    "    fr = open(data_dir, \"r\")\n",
    "    ll = fr.readlines()\n",
    "    seq_list = []\n",
    "    label_list = []\n",
    "    print(\"part:{}\".format(part))\n",
    "    for i in range(1, len(ll)):\n",
    "        tmp_list = ll[i].strip().split(\"\\t\")\n",
    "        seq_list.append(tmp_list[0])\n",
    "        label_list.append(float(tmp_list[1]))\n",
    "        \n",
    "    tem_part = int((len(seq_list)/10)*part)\n",
    "    \n",
    "    seq_list = seq_list[:tem_part]\n",
    "    label_list =label_list[:tem_part]\n",
    "    seq_np = np.array(\n",
    "        [string_to_one_hot(seq, AAS) for seq in seq_list]\n",
    "    )\n",
    "\n",
    "    labels = torch.from_numpy(np.array(label_list))\n",
    "    labels = labels.to(torch.float32)\n",
    "    one_hots = torch.from_numpy(seq_np)\n",
    "    one_hots = one_hots.to(torch.float32)\n",
    "    return one_hots, labels\n",
    "\n",
    "def train_cnn_predictor(data_dir,part=1):\n",
    "    \n",
    "    one_hots, labels = raw_to_features(data_dir,part)\n",
    "    seq_dataset = MyDataset(one_hots, labels)\n",
    "    index = [8,8,10,10,10,10,5,5,5,5,5]\n",
    "    epochs = index[part]\n",
    "    train_loader = DataLoader(\n",
    "        seq_dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    model = CNN2(\n",
    "        len(pab1_wt_sequence),\n",
    "        len(AAS),).to('cuda')\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs = batch[0]\n",
    "            inputs = inputs.permute(0,2,1).to('cuda')\n",
    "            labels = batch[1].to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(inputs)\n",
    "            logits = logits.squeeze()\n",
    "            loss = F.mse_loss(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(\"train_loss:{}\".format(loss))\n",
    "    return model\n",
    "class TrainPipeline():\n",
    "    def __init__(self, start_seq, alphabet, model, trust_radius, init_model=None): #init_model=None\n",
    "        self.seq_len = len(start_seq)\n",
    "        self.vocab_size = len(alphabet)\n",
    "        self.n_in_row = 4\n",
    "        self.seq_env = Seq_env(\n",
    "            self.seq_len,\n",
    "            alphabet,\n",
    "            model,\n",
    "            start_seq,\n",
    "            trust_radius)  #n_in_row=self.n_in_row\n",
    "        self.mutate = Mutate(self.seq_env)\n",
    "        # training params\n",
    "        self.learn_rate = 2e-3\n",
    "        self.lr_multiplier = 1.0  # adaptively adjust the learning rate based on KL\n",
    "        self.temp = 1.0  # the temperature param\n",
    "        self.n_playout = 200  # num of simulations for each move 400   1600\n",
    "        self.c_puct = 10 #0.5  # 10\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 32  # mini-batch size for training  512\n",
    "        self.data_buffer = deque(maxlen=self.buffer_size)\n",
    "        self.play_batch_size = 1\n",
    "        self.epochs = 5  # num of train_steps for each update\n",
    "        self.kl_targ = 0.02\n",
    "        self.check_freq = 50\n",
    "        self.game_batch_num = 1500\n",
    "        self.best_win_ratio = 0.0\n",
    "        # num of simulations used for the pure mcts, which is used as\n",
    "        # the opponent to evaluate the trained policy\n",
    "        self.pure_mcts_playout_num = 1000\n",
    "        #self_added\n",
    "        self.buffer_no_extend = False\n",
    "        #self_added\n",
    "        #playout\n",
    "        self.generated_seqs = []\n",
    "        self.fit_list = []\n",
    "        self.p_dict = {}\n",
    "        self.m_p_dict = {}\n",
    "        self.retrain_flag = False\n",
    "        self.part = 2\n",
    "        #playout\n",
    "        #\n",
    "        if init_model:\n",
    "            # start training from an initial policy-value net\n",
    "            self.policy_value_net = PolicyValueNet(self.seq_len,\n",
    "                                                   self.vocab_size,\n",
    "                                                   model_file=init_model,use_gpu=True)\n",
    "        else:\n",
    "            # start training from a new policy-value net\n",
    "            self.policy_value_net = PolicyValueNet(self.seq_len,\n",
    "                                                   self.vocab_size,use_gpu=True)\n",
    "        self.mcts_player = MCTSMutater(self.policy_value_net.policy_value_fn,\n",
    "                                      c_puct=self.c_puct,\n",
    "                                      n_playout=self.n_playout,\n",
    "                                      is_selfplay=1)\n",
    "\n",
    "    def collect_selfplay_data(self, n_games=1):\n",
    "        \"\"\"collect self-play data for training\"\"\"\n",
    "        counts = len(self.generated_seqs)\n",
    "        self.buffer_no_extend = False\n",
    "        for i in range(n_games):\n",
    "            play_data, seq_and_fit, p_dict = self.mutate.start_mutating(self.mcts_player,\n",
    "                                                          temp=self.temp)    #winner,\n",
    "            play_data = list(play_data)[:]\n",
    "            self.episode_len = len(play_data)\n",
    "\n",
    "            self.p_dict = p_dict\n",
    "            self.m_p_dict.update(self.p_dict)\n",
    "            if self.episode_len == 0:\n",
    "                self.buffer_no_extend = True\n",
    "            else:\n",
    "                self.data_buffer.extend(play_data)\n",
    "                for seq, fit in seq_and_fit:  #alphafold_d\n",
    "                    if seq not in self.generated_seqs:\n",
    "                        self.generated_seqs.append(seq)\n",
    "                        self.fit_list.append(fit)\n",
    "                        if seq not in self.m_p_dict.keys():\n",
    "                            self.m_p_dict[seq] = fit\n",
    "                    \n",
    "                        if len(self.generated_seqs)%10==0 and len(self.generated_seqs)>counts and self.part<=10:\n",
    "                            self.retrain_flag=True\n",
    "                       \n",
    "\n",
    "    def policy_update(self):\n",
    "        \"\"\"update the policy-value net\"\"\"\n",
    "        mini_batch = random.sample(self.data_buffer, self.batch_size)\n",
    "        state_batch = [data[0] for data in mini_batch]\n",
    "        mcts_probs_batch = [data[1] for data in mini_batch]\n",
    "        winner_batch = [data[2] for data in mini_batch]\n",
    "        old_probs, old_v = self.policy_value_net.policy_value(state_batch)\n",
    "        for i in range(self.epochs):\n",
    "            loss, entropy = self.policy_value_net.train_step(\n",
    "                    state_batch,\n",
    "                    mcts_probs_batch,\n",
    "                    winner_batch,\n",
    "                    self.learn_rate*self.lr_multiplier)\n",
    "            new_probs, new_v = self.policy_value_net.policy_value(state_batch)\n",
    "            kl = np.mean(np.sum(old_probs * (\n",
    "                    np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\n",
    "                    axis=1)\n",
    "            )\n",
    "            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n",
    "                break\n",
    "        # adaptively adjust the learning rate\n",
    "        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\n",
    "            self.lr_multiplier /= 1.5\n",
    "        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n",
    "            self.lr_multiplier *= 1.5\n",
    "\n",
    "        explained_var_old = (1 -\n",
    "                             np.var(np.array(winner_batch) - old_v.flatten()) /\n",
    "                             np.var(np.array(winner_batch)))\n",
    "        explained_var_new = (1 -\n",
    "                             np.var(np.array(winner_batch) - new_v.flatten()) /\n",
    "                             np.var(np.array(winner_batch)))\n",
    "        print((\"kl:{:.5f},\"\n",
    "               \"lr_multiplier:{:.3f},\"\n",
    "               \"loss:{},\"\n",
    "               \"entropy:{},\"\n",
    "               \"explained_var_old:{:.3f},\"\n",
    "               \"explained_var_new:{:.3f}\"\n",
    "               ).format(kl,\n",
    "                        self.lr_multiplier,\n",
    "                        loss,\n",
    "                        entropy,\n",
    "                        explained_var_old,\n",
    "                        explained_var_new))\n",
    "        return loss, entropy\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"run the training pipeline\"\"\"\n",
    "        starttime = datetime.datetime.now() \n",
    "        #part =2\n",
    "        try:\n",
    "            for i in range(self.game_batch_num):\n",
    "                self.collect_selfplay_data(self.play_batch_size)\n",
    "                print(\"batch i:{}, episode_len:{}\".format(\n",
    "                        i+1, self.episode_len))\n",
    "                if self.retrain_flag and self.part<=10:\n",
    "                    print('train predictor again')\n",
    "\n",
    "                    update_model = train_cnn_predictor(data_dir,self.part)\n",
    "                    \n",
    "                    self.seq_env.model = update_model\n",
    "                    self.seq_env.model.eval()\n",
    "                    self.part = self.part+1\n",
    "                    self.retrain_flag = False\n",
    "                if len(self.m_p_dict.keys()) >= 4000:\n",
    "                    m_p_fitness = np.array(list(self.m_p_dict.values()))\n",
    "                    m_p_seqs = np.array(list(self.m_p_dict.keys()))\n",
    "                    df_m_p = pd.DataFrame(\n",
    "                        {\"sequence\": m_p_seqs, \"pred_fit\": m_p_fitness})\n",
    "                    df_m_p.to_csv(r\"./evozero_pab1_generated_sequence_1.csv\",index=False)\n",
    "                    endtime = datetime.datetime.now() \n",
    "                    print('time cost：',(endtime-starttime).seconds)\n",
    "                    sys.exit(0)\n",
    "                if len(self.data_buffer) > self.batch_size and self.buffer_no_extend == False:\n",
    "                    loss, entropy = self.policy_update()\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\n\\rquit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2c5d38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T08:07:59.127387Z",
     "start_time": "2023-06-09T07:53:25.007763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd:  nvidia-smi -q -d Memory |grep -A4 GPU|grep Free\n",
      "        Free                              : 4165 MiB\n",
      "        Free                              : 4213 MiB\n",
      "        Free                              : 4213 MiB\n",
      "        Free                              : 4253 MiB\n",
      "        Free                              : 45634 MiB\n",
      "        Free                              : 45634 MiB\n",
      "        Free                              : 45634 MiB\n",
      "        Free                              : 45634 MiB\n",
      "\n",
      "['0:4165', '1:4213', '2:4213', '3:4253', '4:45634', '5:45634', '6:45634', '7:45634']\n",
      "max remain memory GPU: 4\n",
      "part:1\n",
      "train_loss:0.10480683296918869\n",
      "起始序列：GNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKEAIDALKGMLLNGQEIYFAP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/alldata/Htang_data/workspace/project/EvoZero_PAB1_GFP_task_clear/PAB1_GFP_task/p_v_net_3.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x_act = F.log_softmax(self.act_fc1(x_act))\n",
      "/home/Htang/miniconda3/envs/lm_gvp/lib/python3.6/site-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI move: 1245\n",
      "\n",
      "move_fitness: 0.528081\n",
      "\n",
      "episode_seq len: 2\n",
      "\n",
      "Mmove & playout dict len: 199\n",
      "\n",
      "GNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKEAIDALKMMLLNGQEIYFAP\n",
      "AI move: 170\n",
      "\n",
      "move_fitness: 0.568954\n",
      "\n",
      "episode_seq len: 3\n",
      "\n",
      "Mmove & playout dict len: 398\n",
      "\n",
      "GNIFIKNLDPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKEAIDALKMMLLNGQEIYFAP\n",
      "AI move: 1374\n",
      "\n",
      "move_fitness: 0.704265\n",
      "\n",
      "episode_seq len: 4\n",
      "\n",
      "Mmove & playout dict len: 593\n",
      "\n",
      "GNIFIKNLDPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKEAIDALKMMLLNGCEIYFAP\n",
      "AI move: 1239\n",
      "\n",
      "move_fitness: 0.676428\n",
      "\n",
      "episode_seq len: 5\n",
      "\n",
      "Mmove & playout dict len: 792\n",
      "\n",
      "GNIFIKNLDPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKEAIDALPMMLLNGCEIYFAP\n",
      "batch i:1, episode_len:4\n",
      "起始序列：GNIFIKNLDPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKEAIDALKMMLLNGCEIYFAP\n",
      "AI move: 1117\n",
      "\n",
      "move_fitness: 0.805845\n",
      "\n",
      "episode_seq len: 6\n",
      "\n",
      "Mmove & playout dict len: 792\n",
      "\n",
      "GNIFIKNLDPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIDALKMMLLNGCEIYFAP\n",
      "AI move: 136\n",
      "\n",
      "move_fitness: 0.752828\n",
      "\n",
      "episode_seq len: 7\n",
      "\n",
      "Mmove & playout dict len: 989\n",
      "\n",
      "GNIFIKKLDPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIDALKMMLLNGCEIYFAP\n",
      "batch i:2, episode_len:2\n",
      "起始序列：GNIFIKNLDPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIDALKMMLLNGCEIYFAP\n",
      "AI move: 1176\n",
      "\n",
      "move_fitness: 0.828477\n",
      "\n",
      "episode_seq len: 8\n",
      "\n",
      "Mmove & playout dict len: 989\n",
      "\n",
      "GNIFIKNLDPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAP\n",
      "AI move: 1491\n",
      "\n",
      "move_fitness: 0.833866\n",
      "\n",
      "episode_seq len: 9\n",
      "\n",
      "Mmove & playout dict len: 1186\n",
      "\n",
      "GNIFIKNLDPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 198\n",
      "\n",
      "move_fitness: 0.884658\n",
      "\n",
      "episode_seq len: 10\n",
      "\n",
      "Mmove & playout dict len: 1384\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 890\n",
      "\n",
      "move_fitness: 0.737939\n",
      "\n",
      "episode_seq len: 11\n",
      "\n",
      "Mmove & playout dict len: 1580\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGDVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "batch i:3, episode_len:4\n",
      "train predictor again\n",
      "part:2\n",
      "train_loss:0.020704016089439392\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 468\n",
      "\n",
      "move_fitness: 0.816968\n",
      "\n",
      "episode_seq len: 12\n",
      "\n",
      "Mmove & playout dict len: 1580\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTFSVWGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "batch i:4, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 633\n",
      "\n",
      "move_fitness: 0.887529\n",
      "\n",
      "episode_seq len: 13\n",
      "\n",
      "Mmove & playout dict len: 1580\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTFSVFGDILSSKHATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "batch i:5, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 854\n",
      "\n",
      "move_fitness: 0.494090\n",
      "\n",
      "episode_seq len: 14\n",
      "\n",
      "Mmove & playout dict len: 1581\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGCGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "batch i:6, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 1047\n",
      "\n",
      "move_fitness: 0.940010\n",
      "\n",
      "episode_seq len: 15\n",
      "\n",
      "Mmove & playout dict len: 1582\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGYAKSAIKALKMMLLNGCEIYFAQ\n",
      "batch i:7, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 225\n",
      "\n",
      "move_fitness: 0.850153\n",
      "\n",
      "episode_seq len: 16\n",
      "\n",
      "Mmove & playout dict len: 1583\n",
      "\n",
      "GNIFIKNLDTDMDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "batch i:8, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 1262\n",
      "\n",
      "move_fitness: 0.918612\n",
      "\n",
      "episode_seq len: 17\n",
      "\n",
      "Mmove & playout dict len: 1584\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMVLLNGCEIYFAQ\n",
      "batch i:9, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 1261\n",
      "\n",
      "move_fitness: 0.856894\n",
      "\n",
      "episode_seq len: 18\n",
      "\n",
      "Mmove & playout dict len: 1585\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMLLLNGCEIYFAQ\n",
      "batch i:10, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 587\n",
      "\n",
      "move_fitness: 0.787440\n",
      "\n",
      "episode_seq len: 19\n",
      "\n",
      "Mmove & playout dict len: 1586\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTFSVFGDILSYKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "batch i:11, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 671\n",
      "\n",
      "move_fitness: 1.020496\n",
      "\n",
      "episode_seq len: 20\n",
      "\n",
      "Mmove & playout dict len: 1587\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIAQDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "batch i:12, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 917\n",
      "\n",
      "move_fitness: 0.326770\n",
      "\n",
      "episode_seq len: 21\n",
      "\n",
      "Mmove & playout dict len: 1588\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFSHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "batch i:13, episode_len:1\n",
      "train predictor again\n",
      "part:3\n",
      "train_loss:0.027435816824436188\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 402\n",
      "\n",
      "move_fitness: 1.294991\n",
      "\n",
      "episode_seq len: 22\n",
      "\n",
      "Mmove & playout dict len: 1588\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIKALKMMLLNGCEIYFAQ\n",
      "AI move: 1175\n",
      "\n",
      "move_fitness: 1.334348\n",
      "\n",
      "episode_seq len: 23\n",
      "\n",
      "Mmove & playout dict len: 1784\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "AI move: 793\n",
      "\n",
      "move_fitness: 1.390770\n",
      "\n",
      "episode_seq len: 24\n",
      "\n",
      "Mmove & playout dict len: 1979\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "AI move: 1139\n",
      "\n",
      "move_fitness: 0.722776\n",
      "\n",
      "episode_seq len: 25\n",
      "\n",
      "Mmove & playout dict len: 2176\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSPIRALKMMLLNGCEIYFAQ\n",
      "batch i:14, episode_len:4\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "AI move: 258\n",
      "\n",
      "move_fitness: 1.230767\n",
      "\n",
      "episode_seq len: 26\n",
      "\n",
      "Mmove & playout dict len: 2185\n",
      "\n",
      "GNIFIKNLDTDITNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "batch i:15, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "AI move: 167\n",
      "\n",
      "move_fitness: 1.239179\n",
      "\n",
      "episode_seq len: 27\n",
      "\n",
      "Mmove & playout dict len: 2185\n",
      "\n",
      "GNIFIKNLYTDIDNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "batch i:16, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "AI move: 1076\n",
      "\n",
      "move_fitness: 1.347054\n",
      "\n",
      "episode_seq len: 28\n",
      "\n",
      "Mmove & playout dict len: 2185\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAKKSAIRALKMMLLNGCEIYFAQ\n",
      "batch i:17, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "AI move: 590\n",
      "\n",
      "move_fitness: 1.096784\n",
      "\n",
      "episode_seq len: 29\n",
      "\n",
      "Mmove & playout dict len: 2185\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDILSDKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "batch i:18, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "AI move: 283\n",
      "\n",
      "move_fitness: 1.312234\n",
      "\n",
      "episode_seq len: 30\n",
      "\n",
      "Mmove & playout dict len: 2185\n",
      "\n",
      "GNIFIKNLDTDIDNAALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "batch i:19, episode_len:1\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYFAQ\n",
      "AI move: 1443\n",
      "\n",
      "move_fitness: 1.912243\n",
      "\n",
      "episode_seq len: 31\n",
      "\n",
      "Mmove & playout dict len: 2185\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDILSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYAAQ\n",
      "AI move: 545\n",
      "\n",
      "move_fitness: 2.026827\n",
      "\n",
      "episode_seq len: 32\n",
      "\n",
      "Mmove & playout dict len: 2381\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDIMSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMLLNGCEIYAAQ\n",
      "AI move: 1297\n",
      "\n",
      "move_fitness: 2.307068\n",
      "\n",
      "episode_seq len: 33\n",
      "\n",
      "Mmove & playout dict len: 2581\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDIMSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMSLNGCEIYAAQ\n",
      "AI move: 854\n",
      "\n",
      "move_fitness: 1.749997\n",
      "\n",
      "episode_seq len: 34\n",
      "\n",
      "Mmove & playout dict len: 2777\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDIMSSKIATDENGKHKGCGFVHFEEEGAAKSAIRALKMMSLNGCEIYAAQ\n",
      "batch i:20, episode_len:4\n",
      "train predictor again\n",
      "part:4\n",
      "train_loss:0.061669304966926575\n",
      "kl:0.38351,lr_multiplier:0.667,loss:8.017382621765137,entropy:7.270013809204102,explained_var_old:0.001,explained_var_new:0.003\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTVSVFGDIMSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMSLNGCEIYAAQ\n",
      "AI move: 1479\n",
      "\n",
      "move_fitness: 2.116622\n",
      "\n",
      "episode_seq len: 35\n",
      "\n",
      "Mmove & playout dict len: 2787\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDIMSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMSLNGCEIYAPQ\n",
      "AI move: 1004\n",
      "\n",
      "move_fitness: 1.458841\n",
      "\n",
      "episode_seq len: 36\n",
      "\n",
      "Mmove & playout dict len: 2983\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDIMSSKIATDENGKHKGFGFVHFEEGGAAKSAIRALKMMSLNGCEIYAPQ\n",
      "batch i:21, episode_len:2\n",
      "kl:0.25454,lr_multiplier:0.444,loss:7.020834922790527,entropy:6.824389457702637,explained_var_old:0.002,explained_var_new:0.003\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTVSVFGDIMSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMSLNGCEIYAPQ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI move: 914\n",
      "\n",
      "move_fitness: 1.323082\n",
      "\n",
      "episode_seq len: 37\n",
      "\n",
      "Mmove & playout dict len: 2989\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDIMSSKIATDENGKHKGFGFCHFEEEGAAKSAIRALKMMSLNGCEIYAPQ\n",
      "batch i:22, episode_len:1\n",
      "kl:0.09130,lr_multiplier:0.296,loss:6.331491947174072,entropy:6.018428325653076,explained_var_old:0.002,explained_var_new:0.002\n",
      "起始序列：GNIFIKNLDTDIDNKALYDTVSVFGDIMSSKIATDENGKHKGFGFVHFEEEGAAKSAIRALKMMSLNGCEIYAPQ\n",
      "AI move: 1007\n",
      "\n",
      "move_fitness: 1.427140\n",
      "\n",
      "episode_seq len: 38\n",
      "\n",
      "Mmove & playout dict len: 2989\n",
      "\n",
      "GNIFIKNLDTDIDNKALYDTVSVFGDIMSSKIATDENGKHKGFGFVHFEEYGAAKSAIRALKMMSLNGCEIYAPQ\n",
      "AI move: 378\n",
      "\n",
      "move_fitness: 1.593732\n",
      "\n",
      "episode_seq len: 39\n",
      "\n",
      "Mmove & playout dict len: 3185\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEEYGAAKSAIRALKMMSLNGCEIYAPQ\n",
      "AI move: 1022\n",
      "\n",
      "move_fitness: 1.645805\n",
      "\n",
      "episode_seq len: 40\n",
      "\n",
      "Mmove & playout dict len: 3381\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEEYVAAKSAIRALKMMSLNGCEIYAPQ\n",
      "AI move: 1108\n",
      "\n",
      "move_fitness: 1.622779\n",
      "\n",
      "episode_seq len: 41\n",
      "\n",
      "Mmove & playout dict len: 3580\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEEYVAAKWAIRALKMMSLNGCEIYAPQ\n",
      "batch i:23, episode_len:4\n",
      "train predictor again\n",
      "part:5\n",
      "train_loss:0.03116539493203163\n",
      "kl:0.10713,lr_multiplier:0.198,loss:5.68046236038208,entropy:5.360107421875,explained_var_old:0.003,explained_var_new:0.001\n",
      "起始序列：GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEEYVAAKSAIRALKMMSLNGCEIYAPQ\n",
      "AI move: 1145\n",
      "\n",
      "move_fitness: 1.734920\n",
      "\n",
      "episode_seq len: 42\n",
      "\n",
      "Mmove & playout dict len: 3581\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEEYVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "AI move: 1115\n",
      "\n",
      "move_fitness: 1.108088\n",
      "\n",
      "episode_seq len: 43\n",
      "\n",
      "Mmove & playout dict len: 3776\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEEYVAAKRAMRALKMMSLNGCEIYAPQ\n",
      "batch i:24, episode_len:2\n",
      "kl:0.01366,lr_multiplier:0.198,loss:5.628060340881348,entropy:5.333909511566162,explained_var_old:0.001,explained_var_new:0.000\n",
      "起始序列：GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEEYVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "AI move: 1015\n",
      "\n",
      "move_fitness: 1.412328\n",
      "\n",
      "episode_seq len: 44\n",
      "\n",
      "Mmove & playout dict len: 3777\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "AI move: 1156\n",
      "\n",
      "move_fitness: 1.378605\n",
      "\n",
      "episode_seq len: 45\n",
      "\n",
      "Mmove & playout dict len: 3971\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEERVAAKSAKRALKMMSLNGCEIYAPQ\n",
      "batch i:25, episode_len:2\n",
      "kl:0.01535,lr_multiplier:0.198,loss:5.615490436553955,entropy:5.354957580566406,explained_var_old:0.000,explained_var_new:0.000\n",
      "起始序列：GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "AI move: 1261\n",
      "\n",
      "move_fitness: 1.391934\n",
      "\n",
      "episode_seq len: 46\n",
      "\n",
      "Mmove & playout dict len: 3973\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEERVAAKSAMRALKMLSLNGCEIYAPQ\n",
      "batch i:26, episode_len:1\n",
      "kl:0.01203,lr_multiplier:0.198,loss:5.632820129394531,entropy:5.360053062438965,explained_var_old:0.000,explained_var_new:0.000\n",
      "起始序列：GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "AI move: 1239\n",
      "\n",
      "move_fitness: 1.225738\n",
      "\n",
      "episode_seq len: 47\n",
      "\n",
      "Mmove & playout dict len: 3974\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEERVAAKSAMRALPMMSLNGCEIYAPQ\n",
      "batch i:27, episode_len:1\n",
      "kl:0.00703,lr_multiplier:0.296,loss:5.553489685058594,entropy:5.366670608520508,explained_var_old:0.000,explained_var_new:0.000\n",
      "起始序列：GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "AI move: 593\n",
      "\n",
      "move_fitness: 1.264365\n",
      "\n",
      "episode_seq len: 48\n",
      "\n",
      "Mmove & playout dict len: 3976\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSHKIATDENGKHKGFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "batch i:28, episode_len:1\n",
      "kl:0.01224,lr_multiplier:0.296,loss:5.6284661293029785,entropy:5.3649396896362305,explained_var_old:0.000,explained_var_new:0.000\n",
      "起始序列：GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "AI move: 823\n",
      "\n",
      "move_fitness: 1.340830\n",
      "\n",
      "episode_seq len: 49\n",
      "\n",
      "Mmove & playout dict len: 3976\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKAFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "batch i:29, episode_len:1\n",
      "kl:0.00967,lr_multiplier:0.444,loss:5.646329402923584,entropy:5.364831924438477,explained_var_old:0.000,explained_var_new:0.000\n",
      "起始序列：GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "AI move: 602\n",
      "\n",
      "move_fitness: 0.781700\n",
      "\n",
      "episode_seq len: 50\n",
      "\n",
      "Mmove & playout dict len: 3976\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSVIATDENGKHKGFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "batch i:30, episode_len:1\n",
      "kl:0.01024,lr_multiplier:0.444,loss:5.6168951988220215,entropy:5.369259834289551,explained_var_old:0.000,explained_var_new:0.000\n",
      "起始序列：GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENGKHKGFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "AI move: 759\n",
      "\n",
      "move_fitness: 1.417334\n",
      "\n",
      "episode_seq len: 51\n",
      "\n",
      "Mmove & playout dict len: 3977\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTVSVFGDIMSSKIATDENPKHKGFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "AI move: 407\n",
      "\n",
      "move_fitness: 1.274635\n",
      "\n",
      "episode_seq len: 52\n",
      "\n",
      "Mmove & playout dict len: 4173\n",
      "\n",
      "GNIFIKNLDTDIDNKALYTTYSVFGDIMSSKIATDENPKHKGFGFVHFEERVAAKSAMRALKMMSLNGCEIYAPQ\n",
      "batch i:31, episode_len:2\n",
      "train predictor again\n",
      "part:6\n",
      "train_loss:0.03434407711029053\n",
      "time cost： 869\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Htang/miniconda3/envs/lm_gvp/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cmd = \"nvidia-smi -q -d Memory |grep -A4 GPU|grep Free\"\n",
    "print('cmd: ', cmd)\n",
    "process = os.popen(cmd) # return file\n",
    "output = process.read()\n",
    "print(output)\n",
    "process.close()\n",
    "memory_gpu=[int(x.split()[2]) for x in output.strip().split('\\n')]\n",
    "print([str(ii)+':'+str(v) for ii, v in enumerate(memory_gpu)])\n",
    "print('max remain memory GPU:', str(np.argmax(memory_gpu)))\n",
    "# set gpu env\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=str(np.argmax(memory_gpu))\n",
    "\n",
    "\n",
    "data_dir = '/home/Htang/workspace/project/EvoZero_PAB1_GFP_task_clear/PAB1_GFP_data/PAB1.txt'\n",
    "\n",
    "pab1_wt_sequence = (\n",
    "        \"GNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKEAIDALNGMLLNGQEIYVAP\"\n",
    "    )\n",
    "starts = {\n",
    "        \"start_seq\": \"GNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKEAIDALKGMLLNGQEIYFAP\" # noqa: E501\n",
    "    }\n",
    "AAS = \"ILVAGMFYWEDQNHCRKSTP\"\n",
    "\n",
    "\n",
    "starttime = datetime.datetime.now() \n",
    "model = train_cnn_predictor(data_dir)\n",
    "training_pipeline = TrainPipeline(\n",
    "    starts[\"start_seq\"], \n",
    "    AAS,\n",
    "    model,\n",
    "    trust_radius=100,\n",
    ")\n",
    "training_pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_gvp",
   "language": "python",
   "name": "lm_gvp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
